{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core.envs.grid_world import Grid_World\n",
    "from core.rl_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 2 \n",
      "Number of actions: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALEAAACxCAYAAACLKVzFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAACXElEQVR4nO3dsU0DQRBAUQ5RAhEx5E6pgE4oihbISSiAVgic0ICXBtAh4dMeX34vtKXzyvqa4KTVLGOMKyi73vsAcC4Rkydi8kRMnojJEzF5N2tfnj4fNn3/9nR32PJxXJj30+vy0+cmMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkiJm/1etIlXic6Pj/ufYRf3b587H2Ef8UkJk/E5ImYPBGTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXmryxi3Vlh0WFD4H2cujDSJyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETN7UZYwzF/T9lUWHPSYxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyZu6jLHAosNtfL3dT/stk5g8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETN4yxtj7DHAWk5g8EZMnYvJETJ6IyRMxed+WYB02Wi/dVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Grid_World()\n",
    "env.reset()\n",
    "env.render();\n",
    "print('State dimension:', env.state_dim, '\\nNumber of actions:', env.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        self.input_dim = state_dim         # Input dimension\n",
    "        self.output_dim = num_actions      # Output dimension\n",
    "        self.learning_rate = 0.1           # Learning rate for SGD\n",
    "        \n",
    "        self.n1 = n1                       # Number of neurons in the first layer\n",
    "        self.n2 = self.output_dim          # Number of neurons in the second layer\n",
    "        \n",
    "        self.W1 = np.random.normal(loc=0, scale=0.01, size=(self.n1, self.input_dim))  # Weights of the first layer\n",
    "        self.b1 = np.zeros((self.n1, 1))                                               # Biases of the first layer\n",
    "        self.W2 = np.random.normal(loc=0, scale=0.01, size=(self.n2, self.n1))         # Weights of the second layer\n",
    "        self.b2 = np.zeros((self.n2, 1))                                               # Biases of the second layer\n",
    "        \n",
    "    def f2(self, q, compute_derivative=False): # Activation function of layer 2\n",
    "        y = q                                  # Linear function\n",
    "        dydq = 0                               # Default derivative\n",
    "        if(compute_derivative):\n",
    "            dydq = np.ones_like(q)             # Derivative computation\n",
    "        return y, dydq\n",
    "        \n",
    "    def f1(self, q, compute_derivative=False): # Activation function of layer 1\n",
    "        y = 1/(1 + np.exp(-q))                 # Sigmoid function\n",
    "        dydq = 0                               # Default derivative\n",
    "        if(compute_derivative):\n",
    "            dydq = y*(1 - y)                   # Derivative computation\n",
    "        return y, dydq\n",
    "\n",
    "    def forward_pass(self, X, track_derivatives=False):     # track_derivatives must be True when training\n",
    "        self.X_in = X.reshape(self.input_dim, -1)           # Check shape\n",
    "        self.ones_batch = np.ones((self.X_in.shape[1], 1))  # Column vector of ones with the size of the input batch\n",
    "        \n",
    "        Q1 = np.dot(self.W1, self.X_in) + np.dot(self.b1, self.ones_batch.T)      # Linear combination in first layer \n",
    "        self.Y1, self.dY1dQ1 = self.f1(Q1, compute_derivative=track_derivatives)  # Output of the first layer\n",
    "        \n",
    "        Q2 = np.dot(self.W2, self.Y1) + np.dot(self.b2, self.ones_batch.T)        # Linear combination in second layer \n",
    "        self.Y2, self.dY2dQ2 = self.f2(Q2, compute_derivative=track_derivatives)  # Output of the second layer\n",
    "        \n",
    "        return self.Y2.copy() # Return the output of the network\n",
    "        \n",
    "    def backward_pass(self, initial_gradient): # Must be called after forward_pass()\n",
    "        self.dLdY2 = initial_gradient.reshape(self.n2, -1)   # Check shape\n",
    "        Phi2 = self.dLdY2 * self.dY2dQ2                      # Compute the Phi2 matrix      \n",
    "        dLdY1 = np.dot(self.W2.T, Phi2)                      # Compute the dLdY1 matrix\n",
    "        Phi1 = dLdY1 * self.dY1dQ1                           # Compute the Phi1 matrix\n",
    "        \n",
    "        self.dLdW1 = np.dot(Phi1, self.X_in.T)     # Compute derivatives of L w.r.t W1\n",
    "        self.dLdb1 = np.dot(Phi1, self.ones_batch) # Compute derivatives of L w.r.t. b1\n",
    "        self.dLdW2 = np.dot(Phi2, self.Y1.T)       # Compute derivatives of L w.r.t. W2\n",
    "        self.dLdb2 = np.dot(Phi2, self.ones_batch) # Compute derivatives of L w.r.t. b2\n",
    "        \n",
    "    def update_parameters(self): # Applies one gradient descent step to the parameters\n",
    "        batch_size = np.sum(self.ones_batch)                              # Compute the batch_size\n",
    "        self.W1 = self.W1 - (self.learning_rate/batch_size) * self.dLdW1  # Update W1 using SGD\n",
    "        self.b1 = self.b1 - (self.learning_rate/batch_size) * self.dLdb1  # Update b1 using SGD\n",
    "        self.W2 = self.W2 - (self.learning_rate/batch_size) * self.dLdW2  # Update W2 using SGD\n",
    "        self.b2 = self.b2 - (self.learning_rate/batch_size) * self.dLdb2  # Update b2 using SGD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
